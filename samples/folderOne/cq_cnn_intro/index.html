<!DOCTYPE html><html class="hide-aside" lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Cq_CNN_Intro | TechBits</title><meta name="keywords" content="AI,Machine Learning,Deep Learning,Software Engineering,Data Science,Programming"><meta name="author" content="Jack Zhao"><meta name="copyright" content="Jack Zhao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Some sources Sources: A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way | by Sumit Saha | Towards Data Science vdumoulin&#x2F;conv_arithmetic: A technical report on convolution arithmet"><meta property="og:type" content="article"><meta property="og:title" content="Cq_CNN_Intro"><meta property="og:url" content="https://bzhao2718.github.io/JackBlog.github.io/samples/folderOne/cq_cnn_intro/index.html"><meta property="og:site_name" content="TechBits"><meta property="og:description" content="Some sources Sources: A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way | by Sumit Saha | Towards Data Science vdumoulin&#x2F;conv_arithmetic: A technical report on convolution arithmet"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.pixabay.com/photo/2021/09/09/04/38/binary-6609473_1280.jpg"><meta property="article:published_time" content="2021-10-11T00:04:01.000Z"><meta property="article:modified_time" content="2021-10-17T00:56:30.364Z"><meta property="article:author" content="Jack Zhao"><meta property="article:tag" content="AI"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Software Engineering"><meta property="article:tag" content="Data Science"><meta property="article:tag" content="Programming"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.pixabay.com/photo/2021/09/09/04/38/binary-6609473_1280.jpg"><link rel="shortcut icon" href="/JackBlog.github.io/img/favicon.png"><link rel="canonical" href="https://bzhao2718.github.io/JackBlog.github.io/samples/folderOne/cq_cnn_intro/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="stylesheet" href="/JackBlog.github.io/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/JackBlog.github.io/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"We didn't find any results for the search: ${query}"}},translate:void 0,noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:500},copy:{success:"Copy successfully",error:"Copy error",noSupport:"The browser does not support"},relativeDate:{homepage:!1,post:!1},runtime:"",date_suffix:{just:"Just",min:"minutes ago",hour:"hours ago",day:"days ago",month:"months ago"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,source:{jQuery:"https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js",justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js",css:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css"},fancybox:{js:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js",css:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"}},isPhotoFigcaption:!0,islazyload:!0,isanchor:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Cq_CNN_Intro",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2021-10-16 19:56:30"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0!==o){const a=new Date;o=864e5*o,o={value:t,expiry:a.getTime()+o};localStorage.setItem(e,JSON.stringify(o))}},get:function(e){var t=localStorage.getItem(e);if(t){t=JSON.parse(t);const o=new Date;if(!(o.getTime()>t.expiry))return t.value;localStorage.removeItem(e)}}},e.getScript=a=>new Promise((t,e)=>{const o=document.createElement("script");o.src=a,o.async=!0,o.onerror=e,o.onload=o.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(o.onload=o.onreadystatechange=null,t())},document.head.appendChild(o)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};e=saveToLocal.get("theme");"dark"===e?activateDarkMode():"light"===e&&activateLightMode();e=saveToLocal.get("aside-status");void 0!==e&&("hide"===e?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));e=saveToLocal.get("global-font-size");void 0!==e&&document.documentElement.style.setProperty("--global-font-size",e+"px");GLOBAL_CONFIG_SITE.isHome&&/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/l-lin/font-awesome-animation/dist/font-awesome-animation.min.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.min.css" media="defer" onload='this.media="all"'><script async src="https://cdn.jsdelivr.net/npm/hexo-butterfly-tag-plugins-plus@latest/lib/carousel-touch.min.js"></script><meta name="generator" content="Hexo 5.4.0"><style>mjx-container[jax=SVG]{direction:ltr}mjx-container[jax=SVG]>svg{overflow:visible}mjx-container[jax=SVG][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=SVG][justify=left]{text-align:left}mjx-container[jax=SVG][justify=right]{text-align:right}g[data-mml-node=merror]>g{fill:red;stroke:red}g[data-mml-node=merror]>rect[data-background]{fill:#ff0;stroke:none}g[data-mml-node=mtable]>line[data-line]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>rect[data-frame]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>.mjx-dashed{stroke-dasharray:140}g[data-mml-node=mtable]>.mjx-dotted{stroke-linecap:round;stroke-dasharray:0,140}g[data-mml-node=mtable]>svg{overflow:visible}[jax=SVG] mjx-tool{display:inline-block;position:relative;width:0;height:0}[jax=SVG] mjx-tool>mjx-tip{position:absolute;top:0;left:0}mjx-tool>mjx-tip{display:inline-block;padding:.2em;border:1px solid #888;font-size:70%;background-color:#f8f8f8;color:#000;box-shadow:2px 2px 5px #aaa}g[data-mml-node=maction][data-toggle]{cursor:pointer}mjx-status{display:block;position:fixed;left:1em;bottom:1em;min-width:25%;padding:.2em .4em;border:1px solid #888;font-size:90%;background-color:#f8f8f8;color:#000}foreignObject[data-mjx-xml]{font-family:initial;line-height:normal;overflow:visible}.MathJax path{stroke-width:3}mjx-container[display=true]{overflow:auto hidden}mjx-container[display=true]+br{display:none}</style></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/JackBlog.github.io/archives/"><div class="headline">Articles</div><div class="length-num">24</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/JackBlog.github.io/tags/"><div class="headline">Tags</div><div class="length-num">8</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/JackBlog.github.io/categories/"><div class="headline">Categories</div><div class="length-num">3</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/JackBlog.github.io/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/JackBlog.github.io/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/JackBlog.github.io/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/JackBlog.github.io/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/JackBlog.github.io/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/JackBlog.github.io/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/JackBlog.github.io/">TechBits</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/JackBlog.github.io/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/JackBlog.github.io/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/JackBlog.github.io/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/JackBlog.github.io/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/JackBlog.github.io/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/JackBlog.github.io/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Cq_CNN_Intro</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-10-11T00:04:01.000Z" title="Created 2021-10-10 19:04:01">2021-10-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-10-17T00:56:30.364Z" title="Updated 2021-10-16 19:56:30">2021-10-16</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>13min</span></span></div></div></div><article class="post-content" id="article-container"><h1 id="some-sources">Some sources</h1><p>Sources:</p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53">A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way | by Sumit Saha | Towards Data Science</a></p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/vdumoulin/conv_arithmetic">vdumoulin/conv_arithmetic: A technical report on convolution arithmetic in the context of deep learning</a></p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional neural network - Wikipedia</a></p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Convolution">Convolution - Wikipedia</a></p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://cs231n.github.io/convolutional-networks/">CS231n Convolutional Neural Networks for Visual Recognition</a></p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://towardsdatascience.com/convolutional-neural-networks-explained-9cc5188c4939">Convolutional Neural Networks, Explained | by Mayank Mishra | Towards Data Science</a></p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/">An Intuitive Explanation of Convolutional Neural Networks – the data science blog</a></p><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721">Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks | by Adam Geitgey | Medium</a></p><h1 id="overview">Overview</h1><p>CNNs use relatively little pre-processing compared to other <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Image_classification">image classification algorithms</a>. This means that the network learns to optimize the <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Filter_(signal_processing)">filters</a> (or kernels) through automated learning, whereas in traditional algorithms these filters are <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Feature_engineering">hand-engineered</a>. This independence from prior knowledge and human intervention in feature extraction is a major advantage.</p><p>The name "convolutional neural network" indicates that the network employs a mathematical operation called <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Convolution">convolution</a>. Convolutional networks are a specialized type of neural networks that use convolution in place of general matrix multiplication in at least one of their layers.</p><p>Here is an inline note. <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p><h1 id="test-cross-reference-external">Test cross reference external</h1><p>Thesis a reference to <a href="../hello-world/#testheaderref">../hello-world/#testheaderref</a> file to the TestHeaderRef header. <em>Not working.</em> The url get is <code>http://localhost:4000/JackBlog.github.io/samples/folderOne/hello-world/#testheaderref</code></p><p>Now use this <a href="../../hello-world/#testheaderref">../../hello-world/#testheaderref</a>. working. But click in Typora won't work.</p><p>Use the root <a href="/samples/hello-world/#testheaderref">/samples/hello-world/#testheaderref</a></p><p>This is a hard code link: <a target="_blank" rel="noopener external nofollow noreferrer" href="http://localhost:4000/JackBlog.github.io/samples/hello-world#testheaderref">hard coded url</a>. Working!</p><p>Now to the <a href="../test_ins_pandoc/#filters">../test_ins_pandoc/#filters</a> part. <em>not working.</em> The url is <code>http://localhost:4000/JackBlog.github.io/samples/folderOne/test_ins_pandoc/#filters</code></p><p>Now add two dots <a href="../../test_ins_pandoc/#filters">../../test_ins_pandoc/#filters</a> working.</p><p>This is to the file in the same folder <a href="../ins_hexo/#notes">../ins_hexo/#notes</a>. Working!</p><p>This is to the file in the subfolder <a href="../folderSubOne/ins_pandoc/#images">../folderSubOne/ins_pandoc/#images</a>. Working!</p><h1 id="convolution-and-cross-correlation">Convolution and Cross-correlation</h1><p>In <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Mathematics">mathematics</a> (in particular, <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Functional_analysis">functional analysis</a>), <strong>convolution</strong> is a <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Operation_(mathematics)">mathematical operation</a> on two <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Function_(mathematics)">functions</a> (<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.464ex" xmlns="http://www.w3.org/2000/svg" width="7.186ex" height="2.059ex" role="img" focusable="false" viewBox="0 -705 3176 910"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"/></g><g data-mml-node="mtext" transform="translate(550,0)"><path data-c="A0" d=""/></g><g data-mml-node="mi" transform="translate(800,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mi" transform="translate(1329,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(1929,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mtext" transform="translate(2449,0)"><path data-c="A0" d=""/></g><g data-mml-node="mi" transform="translate(2699,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"/></g></g></g></svg></mjx-container></span>) that produces a third function <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.566ex" xmlns="http://www.w3.org/2000/svg" width="6.22ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2749.4 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"/></g><g data-mml-node="mo" transform="translate(1161.2,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"/></g><g data-mml-node="mi" transform="translate(1883.4,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"/></g><g data-mml-node="mo" transform="translate(2360.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container></span>that expresses how the shape of one is modified by the other. The term covolution refers to both the result function and to the process of computing it. It is defined as the <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Integral">integral</a> of the product of the two functions after one is reversed and shifted. The integral is evaluated for all values of shift, producing the convolution function. Some features of convolution are similar to <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a>.</p><p>This is ref1 test ref1(2021) <sup class="refplus-num"><a href="#ref-ref1">[2]</a></sup> and this ref2 and ref3 cite both oneTwo (2021) <sup class="refplus-num"><a href="#ref-ref2">[3]</a></sup><sup class="refplus-num"><a href="#ref-ref3">[4]</a></sup></p><p>Cross-correlation</p><p>In <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Signal_processing">signal processing</a>, <strong>cross-correlation</strong> is a <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Similarity_measure">measure of similarity</a> of two series as a function of the displacement of one relative to the other. This is also known as a <em>sliding <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Dot_product">dot product</a></em> or <em>sliding inner-product</em>. It is commonly used for searching a long signal for a shorter, known feature.</p><p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Comparison_convolution_correlation.svg/400px-Comparison_convolution_correlation.svg.png" id="fig:firstFig" alt="Description: first fig text."></p><h1 id="cs231n-notes">CS231n Notes</h1><p>Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply.</p><p>Neural Networks receive an input (a single vector), and transform it through a series of <em>hidden layers</em>. Each hidden layer is made up of a set of neurons, where each neuron is fully connected to all neurons in the previous layer, and where neurons in a single layer function completely independently and do not share any connections. The last fully-connected layer is called the “output layer” and in classification settings it represents the class scores.</p><p>For example, an image of more respectable size, e.g. 200x200x3, would lead to neurons that have 200<em>200</em>3 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.</p><h2 id="conv-layer">Conv Layer</h2><p>The CONV layer’s parameters consist of a set of learnable filters. Every filter is small spatially (along width and height), but extends through the full depth of the input volume. For example, a typical filter on a first layer of a ConvNet might have size 5x5x3 (i.e. 5 pixels width and height, and 3 because images have depth 3, the color channels). During the forward pass, we slide (more precisely, convolve) each filter across the width and height of the input volume and compute dot products between the entries of the filter and the input at any position. As we slide the filter over the width and height of the input volume we will produce a 2-dimensional activation map that gives the responses of that filter at every spatial position. Intuitively, the network will learn filters that activate when they see some type of visual feature such as an edge of some orientation or a blotch of some color on the first layer.</p><p>Now, we will have an entire set of filters in each CONV layer (e.g. 12 filters), and each of them will produce a separate 2-dimensional activation map. We will stack these activation maps along the depth dimension and produce the output volume.</p><p>If you’re a fan of the brain/neuron analogies, every entry in the 3D output volume can also be interpreted as an output of a neuron that looks at only a small region in the input and shares parameters with all neurons to the left and right spatially (since these numbers all result from applying the same filter).</p><p><em>Example 1</em>. For example, suppose that the input volume has size [32x32x3], (e.g. an RGB CIFAR-10 image). If the receptive field (or the filter size) is 5x5, then each neuron in the Conv Layer will have weights to a [5x5x3] region in the input volume, for a total of 5<em>5</em>3 = 75 weights (and +1 bias parameter). Notice that the extent of the connectivity along the depth axis must be 3, since this is the depth of the input volume.</p><p>Three hyperparameters control the size of the output volume: the <strong>depth, stride</strong> and <strong>zero-padding</strong>. We discuss these next:</p><ol type="1"><li>First, the <strong>depth</strong> of the output volume is a hyperparameter: it corresponds to the number of filters we would like to use, each learning to look for something different in the input. For example, if the first Convolutional Layer takes as input the raw image, then different neurons along the depth dimension may activate in presence of various oriented edges, or blobs of color. We will refer to a set of neurons that are all looking at the same region of the input as a <strong>depth column</strong> (some people also prefer the term <em>fibre</em>).</li><li>Second, we must specify the <strong>stride</strong> with which we slide the filter. When the stride is 1 then we move the filters one pixel at a time. When the stride is 2 (or uncommonly 3 or more, though this is rare in practice) then the filters jump 2 pixels at a time as we slide them around. This will produce smaller output volumes spatially.</li><li>As we will soon see, sometimes it will be convenient to pad the input volume with zeros around the border. The size of this <strong>zero-padding</strong> is a hyperparameter. The nice feature of zero padding is that it will allow us to control the spatial size of the output volumes.</li></ol><p><strong>1x1 convolution</strong>. As an aside, several papers use 1x1 convolutions, as first investigated by <a target="_blank" rel="noopener external nofollow noreferrer" href="http://arxiv.org/abs/1312.4400">Network in Network</a>. Some people are at first confused to see 1x1 convolutions especially when they come from signal processing background. Normally signals are 2-dimensional so 1x1 convolutions do not make sense (it’s just pointwise scaling). However, in ConvNets this is not the case because one must remember that we operate over 3-dimensional volumes, and that the filters always extend through the full depth of the input volume. For example, if the input is [32x32x3] then doing 1x1 convolutions would effectively be doing 3-dimensional dot products (since the input depth is 3 channels).</p><p><strong>Dilated convolutions.</strong> A recent development (e.g. see <a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1511.07122">paper by Fisher Yu and Vladlen Koltun</a>) is to introduce one more hyperparameter to the CONV layer called the <em>dilation</em>. So far we’ve only discussed CONV filters that are contiguous. However, it’s possible to have filters that have spaces between each cell, called dilation. As an example, in one dimension a filter <code>w</code> of size 3 would compute over input <code>x</code> the following: <code>w[0]*x[0] + w[1]*x[1] + w[2]*x[2]</code>. This is dilation of 0. For dilation 1 the filter would instead compute <code>w[0]*x[0] + w[1]*x[2] + w[2]*x[4]</code>; In other words there is a gap of 1 between the applications. This can be very useful in some settings to use in conjunction with 0-dilated filters because it allows you to merge spatial information across the inputs much more agressively with fewer layers.</p><h2 id="pooling-layer">Pooling Layer</h2><p><strong>General pooling</strong>. In addition to max pooling, the pooling units can also perform other functions, such as <em>average pooling</em> or even <em>L2-norm pooling</em>. Average pooling was often used historically but has recently fallen out of favor compared to the max pooling operation, which has been shown to work better in practice.</p><p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://cs231n.github.io/assets/cnn/pool.jpeg" style="width:49%"> <img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://cs231n.github.io/assets/cnn/maxpool.jpeg" style="width:49%"></p><p>Pooling layer downsamples the volume spatially, independently in each depth slice of the input volume. <strong>Left:</strong> In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into output volume of size [112x112x64]. Notice that the volume depth is preserved. <strong>Right:</strong> The most common downsampling operation is max, giving rise to <strong>max pooling</strong>, here shown with a stride of 2. That is, each max is taken over 4 numbers (little 2x2 square).</p><p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="media/cq_cnn_intro/conv_slide_eg1.gif" id="fig:giffig" alt="This is a gif."></p><p><strong>Getting rid of pooling</strong>. Many people dislike the pooling operation and think that we can get away without it. For example, <a target="_blank" rel="noopener external nofollow noreferrer" href="http://arxiv.org/abs/1412.6806">Striving for Simplicity: The All Convolutional Net</a> proposes to discard the pooling layer in favor of architecture that only consists of repeated CONV layers. To reduce the size of the representation they suggest using larger stride in CONV layer once in a while. Discarding pooling layers has also been found to be important in training good generative models, such as variational autoencoders (VAEs) or generative adversarial networks (GANs). It seems likely that future architectures will feature very few to no pooling layers.</p><h1 id="comprehensive-guide-notes">Comprehensive Guide Notes</h1><p>A digital image is a binary representation of visual data. It contains a series of pixels arranged in a grid-like fashion that contains pixel values to denote how bright and what color each pixel should be.</p><p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://miro.medium.com/max/874/1*QJtTdtPhikY3KywV44L0Pw.png" style="height:30%" alt="Representation of image as a grid of pixels."></p><p>Essentially, every image can be represented as a matrix of pixel values.</p><p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://miro.medium.com/max/1162/1*zY1qFB9aFfZz66YxxoI2aw.gif" style="height:30%"></p><p>To feed an image into our neural network, we simply treat the 18x18 pixel image as an array of 324 numbers:</p><p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://miro.medium.com/max/2000/1*UDgDe_-GMs4QQbT8UopoGA.png"></p><p>We need to give our neural network understanding of <em>translation invariance</em> — an “8” is an “8” no matter where in the picture it shows up.</p><p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://miro.medium.com/max/2000/1*vkQ0hXDaQv57sALXAJquxA.jpeg" alt="A general CNN flow."></p><p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://miro.medium.com/max/1400/1*uAeANQIOQPqWZnnuH-VEyw.jpeg" alt="A CNN sequence to classify handwritten digits."></p><p>The architecture of a ConvNet is analogous to that of the connectivity pattern of Neurons in the Human Brain and was inspired by the organization of the Visual Cortex. Individual neurons respond to stimuli only in a restricted region of the visual field known as the Receptive Field. A collection of such fields overlap to cover the entire visual area.</p><p><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://miro.medium.com/max/850/1*GLQjM9k0gZ14nYF0XmkRWQ.png" style="height:30%" alt="Flattening of a 3x3 image matrix into a 9x1 vector."></p><p>A CNN typically has three layers: a convolutional layer, a pooling layer, and a fully connected layer.</p><h1 id="reference-test">Reference Test</h1><p>This is to fig1 fig.&nbsp;<strong>¿fig:firstFig?</strong> and fig2 fig.&nbsp;<strong>¿fig:giffig?</strong> over. This is not working.</p><p>Try this <a href="#fig:firstFig">#fig:firstFig</a> over.</p><p>And second gif fig <a href="#fig:giffig">#fig:giffig</a> over.</p><p>This way of referencing figs is actually working, except the figure number is not auto generated, we have to manually set a text.</p><h2 id="referencehead2">ReferenceHead2</h2><p>And some test</p><h2 id="testreferences">testReferences</h2><p>So here is the way what Max C. Foo(2021)<sup class="refplus-num"><a href="#ref-mcf-2021">[1]</a></sup> do.</p><ul id="refplus"><li id="ref-mcf-2021" data-num="1">[1] Max C. Foo. A way to write an article.[J] Journal of Kelaideng University Samwin School. 2021.3 300-321.</li><li id="ref-ref1" data-num="2">[2] Agarwal, Rishabh, Nicholas Frosst, Xuezhou Zhang, Rich Caruana, and Geoffrey E Hinton. 2020. “Neural Additive Models: Interpretable Machine Learning with Neural Nets.” arXiv Preprint arXiv:2004.13912. https://arxiv.org/pdf/ 2004.13912.</li><li id="ref-ref2" data-num="3">[3] Alguliev, Rasim M., Ramiz M. Aliguliyev, and Nijat R. Isazade. 2013. “Multiple Documents Summarization Based on Evolutionary Optimization Algorithm.” Expert Systems with Applications 40 (5): 1675–89. https://doi.org/10.1016/j. eswa.2012.09.014.</li><li id="ref-ref3" data-num="4">[4] Anchieta, Rafael T., Marco A. S. Cabezudo, and Thiago A. S. Pardo. 2019. “SEMA: An Extended Semantic Evaluation Metric for AMR.” arXiv, 1905.12069v1. http://arxiv.org/abs/1905.12069v1.</li></ul><section class="footnotes" role="doc-endnotes"><hr><ol><li id="fn1" role="doc-endnote"><p>Inlines notes are easier to write, since you don't have to pick an identifier and move down to type the note.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section><style>#refplus,#refplus li{padding:0;margin:0;list-style:none}</style><script src="https://unpkg.com/@popperjs/core@2"></script><script src="https://unpkg.com/tippy.js@6"></script><script>document.querySelectorAll(".refplus-num").forEach(e=>{var t=e.firstChild.href.replace(location.origin+location.pathname,"");let r=document.querySelector(t);t=r.dataset.num,t=r.innerText.replace(`[${t}]`,"");tippy(e,{content:t})})</script><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.css"></article><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.pixabay.com/photo/2021/09/09/04/38/binary-6609473_1280.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/JackBlog.github.io/samples/ins_hexo_pandoc_markdown/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://cdn.pixabay.com/photo/2012/10/29/15/38/binary-code-63529_1280.jpg" onerror='onerror=null,src="/JackBlog.github.io/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Ins Hexo Pandoc Markdown</div></div></a></div><div class="next-post pull-right"><a href="/JackBlog.github.io/samples/folderOne/ins_hexo/"><img class="next-cover" src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://cdn.pixabay.com/photo/2012/10/29/15/38/binary-code-63529_1280.jpg" onerror='onerror=null,src="/JackBlog.github.io/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Ins_Hexo File</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#some-sources"><span class="toc-number">1.</span> <span class="toc-text">Some sources</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#overview"><span class="toc-number">2.</span> <span class="toc-text">Overview</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#test-cross-reference-external"><span class="toc-number">3.</span> <span class="toc-text">Test cross reference external</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#convolution-and-cross-correlation"><span class="toc-number">4.</span> <span class="toc-text">Convolution and Cross-correlation</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#cs231n-notes"><span class="toc-number">5.</span> <span class="toc-text">CS231n Notes</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#conv-layer"><span class="toc-number">5.1.</span> <span class="toc-text">Conv Layer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pooling-layer"><span class="toc-number">5.2.</span> <span class="toc-text">Pooling Layer</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#comprehensive-guide-notes"><span class="toc-number">6.</span> <span class="toc-text">Comprehensive Guide Notes</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#reference-test"><span class="toc-number">7.</span> <span class="toc-text">Reference Test</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#referencehead2"><span class="toc-number">7.1.</span> <span class="toc-text">ReferenceHead2</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#testreferences"><span class="toc-number">7.2.</span> <span class="toc-text">testReferences</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Jack Zhao</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="Increase font size"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="Decrease font size"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"></div></div></div><hr><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/JackBlog.github.io/js/utils.js"></script><script src="/JackBlog.github.io/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js").then(()=>{pangu.autoSpacingPage()})}function panguInit(){panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><script src="/JackBlog.github.io/js/search/local-search.js"></script><div class="js-pjax"><script>if(window.MathJax)MathJax.startup.document.state(0),MathJax.texReset(),MathJax.typeset();else{window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"},chtml:{scale:1.2},options:{renderActions:{findScript:[10,t=>{for(const a of document.querySelectorAll('script[type^="math/tex"]')){var e=!!a.type.match(/; *mode=display/);const n=new t.options.MathItem(a.textContent,t.inputJax[0],e);e=document.createTextNode("");a.parentNode.replaceChild(e,a),n.start={node:e,delim:"",n:0},n.end={node:e,delim:"",n:0},t.math.push(n)}},""],insertScript:[200,()=>{document.querySelectorAll("mjx-container:not([display])").forEach(t=>{const e=t.parentNode;("li"===e.nodeName.toLowerCase()?e.parentNode:e).classList.add("has-jax")})},"",!1]}}};const a=document.createElement("script");a.src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js",a.id="MathJax-script",a.async=!0,document.head.appendChild(a)}</script><script>document.getElementsByClassName("mermaid").length&&(window.mermaidJsLoad?mermaid.init():getScript("https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js").then(()=>{window.mermaidJsLoad=!0,mermaid.initialize({theme:"default"})}))</script></div></div><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script></body></html>